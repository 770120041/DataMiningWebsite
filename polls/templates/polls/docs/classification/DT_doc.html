{% extends 'polls/base.html' %}

{% block content %}
    {#Base EXCEL NAV HERE#}
    {% autoescape off %}
        <div class = "container">
        <h1 class="text-primary">DecisionTreeClassifier:</h1>
            <p class="">A decision tree classifier.
            </p>

        <hr>


            <table class="docutils field-list" frame="void" rules="none">
            <colgroup><col class="field-name">
            <col class="field-body">
            </colgroup><tbody valign="top">
            <tr class="field-odd field"><th class="field-name"><h2 class="text-light  font-weight-bold">Parameters:</h2></th><td class="field-body"><dl class="first docutils">
            <dt><strong>criterion</strong> <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”gini”)</span></dt>
            <dd><p class="first last">The function to measure the quality of a split. Supported criteria are
            “gini” for the Gini impurity and “entropy” for the information gain.</p>
            </dd>
            <dt><strong>splitter</strong> <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”best”)</span></dt>
            <dd><p class="first last">The strategy used to choose the split at each node. Supported
            strategies are “best” to choose the best split and “random” to choose
            the best random split.</p>
            </dd>
            <dt><strong>max_depth</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
            <dd><p class="first last">The maximum depth of the tree. If None, then nodes are expanded until
            all leaves are pure or until all leaves contain less than
            min_samples_split samples.</p>
            </dd>
            <dt><strong>min_samples_split</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=2)</span></dt>
            <dd><p class="first">The minimum number of samples required to split an internal node:</p>
            <ul class="simple">
            <li>If int, then consider <cite>min_samples_split</cite> as the minimum number.</li>
            <li>If float, then <cite>min_samples_split</cite> is a fraction and
            <cite>ceil(min_samples_split * n_samples)</cite> are the minimum
            number of samples for each split.</li>
            </ul>
            <div class="last versionchanged">
            <p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
            </div>
            </dd>
            <dt><strong>min_samples_leaf</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=1)</span></dt>
            <dd><p class="first">The minimum number of samples required to be at a leaf node.
            A split point at any depth will only be considered if it leaves at
            least <code class="docutils literal"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
            right branches.  This may have the effect of smoothing the model,
            especially in regression.</p>
            <ul class="simple">
            <li>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</li>
            <li>If float, then <cite>min_samples_leaf</cite> is a fraction and
            <cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
            number of samples for each node.</li>
            </ul>
            <div class="last versionchanged">
            <p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
            </div>
            </dd>
            <dt><strong>min_weight_fraction_leaf</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
            <dd><p class="first last">The minimum weighted fraction of the sum total of weights (of all
            the input samples) required to be at a leaf node. Samples have
            equal weight when sample_weight is not provided.</p>
            </dd>
            <dt><strong>max_features</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=None)</span></dt>
            <dd><p class="first">The number of features to consider when looking for the best split:</p>
            <blockquote>
            <div><ul class="simple">
            <li>If int, then consider <cite>max_features</cite> features at each split.</li>
            <li>If float, then <cite>max_features</cite> is a fraction and
            <cite>int(max_features * n_features)</cite> features are considered at each
            split.</li>
            <li>If “auto”, then <cite>max_features=sqrt(n_features)</cite>.</li>
            <li>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite>.</li>
            <li>If “log2”, then <cite>max_features=log2(n_features)</cite>.</li>
            <li>If None, then <cite>max_features=n_features</cite>.</li>
            </ul>
            </div></blockquote>
            <p class="last">Note: the search for a split does not stop until at least one
            valid partition of the node samples is found, even if it requires to
            effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
            </dd>
            <dt><strong>random_state</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
            <dd><p class="first last">If int, random_state is the seed used by the random number generator;
            If RandomState instance, random_state is the random number generator;
            If None, the random number generator is the RandomState instance used
            by <cite>np.random</cite>.</p>
            </dd>
            <dt><strong>max_leaf_nodes</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
            <dd><p class="first last">Grow a tree with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
            Best nodes are defined as relative reduction in impurity.
            If None then unlimited number of leaf nodes.</p>
            </dd>
            <dt><strong>min_impurity_decrease</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
            <dd><p class="first">A node will be split if this split induces a decrease of the impurity
            greater than or equal to this value.</p>
            <p>The weighted impurity decrease equation is the following:</p>
            <div class="highlight-default" style="position: relative;"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
            </pre></div>
            </div>
            <p>where <code class="docutils literal"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal"><span class="pre">N_t</span></code> is the number of
            samples at the current node, <code class="docutils literal"><span class="pre">N_t_L</span></code> is the number of samples in the
            left child, and <code class="docutils literal"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
            <p><code class="docutils literal"><span class="pre">N</span></code>, <code class="docutils literal"><span class="pre">N_t</span></code>, <code class="docutils literal"><span class="pre">N_t_R</span></code> and <code class="docutils literal"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
            if <code class="docutils literal"><span class="pre">sample_weight</span></code> is passed.</p>
            <div class="last versionadded">
            <p><span class="versionmodified">New in version 0.19.</span></p>
            </div>
            </dd>
            <dt><strong>min_impurity_split</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float, (default=1e-7)</span></dt>
            <dd><p class="first">Threshold for early stopping in tree growth. A node will split
            if its impurity is above the threshold, otherwise it is a leaf.</p>
            <div class="last deprecated">
            <p><span class="versionmodified">Deprecated since version 0.19: </span><code class="docutils literal"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
            <code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
            <code class="docutils literal"><span class="pre">min_impurity_split</span></code> will change from 1e-7 to 0 in 0.23 and it
            will be removed in 0.25. Use <code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> instead.</p>
            </div>
            </dd>
            <dt><strong>class_weight</strong> <span class="classifier-delimiter">:</span> <span class="classifier">dict, list of dicts, “balanced” or None, default=None</span></dt>
            <dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
            If not given, all classes are supposed to have weight one. For
            multi-output problems, a list of dicts can be provided in the same
            order as the columns of y.</p>
            <p>Note that for multioutput (including multilabel) weights should be
            defined for each class of every column in its own dict. For example,
            for four-class multilabel classification weights should be
            [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
            [{1:1}, {2:5}, {3:1}, {4:1}].</p>
            <p>The “balanced” mode uses the values of y to automatically adjust
            weights inversely proportional to class frequencies in the input data
            as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
            <p>For multi-output, the weights of each column of y will be multiplied.</p>
            <p class="last">Note that these weights will be multiplied with sample_weight (passed
            through the fit method) if sample_weight is specified.</p>
            </dd>
            <dt><strong>presort</strong> <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
            <dd><p class="first last">Whether to presort the data to speed up the finding of best splits in
            fitting. For the default settings of a decision tree on large
            datasets, setting this to true may slow down the training process.
            When using either a smaller dataset or a restricted depth, this may
            speed up the training.</p>
            </dd>
            </dl>
            </td>
            </tr>
            <tr class="field-even field"><th class="field-name">Attributes:</th><td class="field-body"><dl class="first last docutils">
            <dt><strong>classes_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_classes] or a list of such arrays</span></dt>
            <dd><p class="first last">The classes labels (single output problem),
            or a list of arrays of class labels (multi-output problem).</p>
            </dd>
            <dt><a class="reference internal" href="#sklearn.tree.DecisionTreeClassifier.feature_importances_" title="sklearn.tree.DecisionTreeClassifier.feature_importances_"><code class="xref py py-obj docutils literal"><span class="pre">feature_importances_</span></code></a> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
            <dd><p class="first last">Return the feature importances.</p>
            </dd>
            <dt><strong>max_features_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int,</span></dt>
            <dd><p class="first last">The inferred value of max_features.</p>
            </dd>
            <dt><strong>n_classes_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int or list</span></dt>
            <dd><p class="first last">The number of classes (for single output problems),
            or a list containing the number of classes for each
            output (for multi-output problems).</p>
            </dd>
            <dt><strong>n_features_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
            <dd><p class="first last">The number of features when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</p>
            </dd>
            <dt><strong>n_outputs_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
            <dd><p class="first last">The number of outputs when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</p>
            </dd>
            <dt><strong>tree_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">Tree object</span></dt>
            <dd><p class="first last">The underlying Tree object. Please refer to
            <code class="docutils literal"><span class="pre">help(sklearn.tree._tree.Tree)</span></code> for attributes of Tree object and
            <a class="reference internal" href="../../auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py"><span class="std std-ref">Understanding the decision tree structure</span></a>
            for basic usage of these attributes.</p>
            </dd>
            </dl>
            </td>
            </tr>
            </tbody>
            </table>

        </div>
    {% endautoescape %}

{% endblock %}