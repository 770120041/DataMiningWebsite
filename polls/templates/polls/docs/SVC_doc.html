{% extends 'polls/base.html' %}

{% block content %}
    {#Base EXCEL NAV HERE#}
    {% autoescape off %}
        <div class = "container">
        <h1 class="text-primary">SVC:</h1>
            <p class="">C-Support Vector Classification.
            </p>
            <p class="">The implementation is based on libsvm. The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.
            </p>
              <p class="">
                  The multiclass support is handled according to a one-vs-one scheme.

For details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.

              </p>
        <hr>

            <table class="docutils field-list" frame="void" rules="none">
            <colgroup><col class="field-name">
            <col class="field-body">
            </colgroup>
            <tbody valign="top">
            <tr class="field-odd field"><th class="field-name"><h2 class="text-light  font-weight-bold">Parameters:</h2></th><td class="field-body"><dl class="first docutils">
            <dt><strong>C</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1.0)</span></dt>
            <dd><p class="first last">Penalty parameter C of the error term.</p>
            </dd>
            <dt><strong>kernel</strong> <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=’rbf’)</span></dt>
            <dd><p class="first last">Specifies the kernel type to be used in the algorithm.
            It must be one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’ or
            a callable.
            If none is given, ‘rbf’ will be used. If a callable is given it is
            used to pre-compute the kernel matrix from data matrices; that matrix
            should be an array of shape <code class="docutils literal"><span class="pre">(n_samples,</span> <span class="pre">n_samples)</span></code>.</p>
            </dd>
            <dt><strong>degree</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=3)</span></dt>
            <dd><p class="first last">Degree of the polynomial kernel function (‘poly’).
            Ignored by all other kernels.</p>
            </dd>
            <dt><strong>gamma</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=’auto’)</span></dt>
            <dd><p class="first">Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’.</p>
            <p class="last">Current default is ‘auto’ which uses 1 / n_features,
            if <code class="docutils literal"><span class="pre">gamma='scale'</span></code> is passed then it uses 1 / (n_features * X.var())
            as value of gamma. The current default of gamma, ‘auto’, will change
            to ‘scale’ in version 0.22. ‘auto_deprecated’, a deprecated version of
            ‘auto’ is used as a default indicating that no explicit value of gamma
            was passed.</p>
            </dd>
            <dt><strong>coef0</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.0)</span></dt>
            <dd><p class="first last">Independent term in kernel function.
            It is only significant in ‘poly’ and ‘sigmoid’.</p>
            </dd>
            <dt><strong>shrinking</strong> <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
            <dd><p class="first last">Whether to use the shrinking heuristic.</p>
            </dd>
            <dt><strong>probability</strong> <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=False)</span></dt>
            <dd><p class="first last">Whether to enable probability estimates. This must be enabled prior
            to calling <cite>fit</cite>, and will slow down that method.</p>
            </dd>
            <dt><strong>tol</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=1e-3)</span></dt>
            <dd><p class="first last">Tolerance for stopping criterion.</p>
            </dd>
            <dt><strong>cache_size</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float, optional</span></dt>
            <dd><p class="first last">Specify the size of the kernel cache (in MB).</p>
            </dd>
            <dt><strong>class_weight</strong> <span class="classifier-delimiter">:</span> <span class="classifier">{dict, ‘balanced’}, optional</span></dt>
            <dd><p class="first last">Set the parameter C of class i to class_weight[i]*C for
            SVC. If not given, all classes are supposed to have
            weight one.
            The “balanced” mode uses the values of y to automatically adjust
            weights inversely proportional to class frequencies in the input data
            as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
            </dd>
            <dt><strong>verbose</strong> <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span></dt>
            <dd><p class="first last">Enable verbose output. Note that this setting takes advantage of a
            per-process runtime setting in libsvm that, if enabled, may not work
            properly in a multithreaded context.</p>
            </dd>
            <dt><strong>max_iter</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=-1)</span></dt>
            <dd><p class="first last">Hard limit on iterations within solver, or -1 for no limit.</p>
            </dd>
            <dt><strong>decision_function_shape</strong> <span class="classifier-delimiter">:</span> <span class="classifier">‘ovo’, ‘ovr’, default=’ovr’</span></dt>
            <dd><p class="first">Whether to return a one-vs-rest (‘ovr’) decision function of shape
            (n_samples, n_classes) as all other classifiers, or the original
            one-vs-one (‘ovo’) decision function of libsvm which has shape
            (n_samples, n_classes * (n_classes - 1) / 2). However, one-vs-one
            (‘ovo’) is always used as multi-class strategy.</p>
            <div class="versionchanged">
            <p><span class="versionmodified">Changed in version 0.19: </span>decision_function_shape is ‘ovr’ by default.</p>
            </div>
            <div class="versionadded">
            <p><span class="versionmodified">New in version 0.17: </span><em>decision_function_shape=’ovr’</em> is recommended.</p>
            </div>
            <div class="last versionchanged">
            <p><span class="versionmodified">Changed in version 0.17: </span>Deprecated <em>decision_function_shape=’ovo’ and None</em>.</p>
            </div>
            </dd>
            <dt><strong>random_state</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
            <dd><p class="first last">The seed of the pseudo random number generator used when shuffling
            the data for probability estimates. If int, random_state is the
            seed used by the random number generator; If RandomState instance,
            random_state is the random number generator; If None, the random
            number generator is the RandomState instance used by <cite>np.random</cite>.</p>
            </dd>
            </dl>
            </td>
            </tr>
            <tr class="field-even field"><th class="field-name">Attributes:</th><td class="field-body"><dl class="first last docutils">
            <dt><strong>support_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV]</span></dt>
            <dd><p class="first last">Indices of support vectors.</p>
            </dd>
            <dt><strong>support_vectors_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, shape = [n_SV, n_features]</span></dt>
            <dd><p class="first last">Support vectors.</p>
            </dd>
            <dt><strong>n_support_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array-like, dtype=int32, shape = [n_class]</span></dt>
            <dd><p class="first last">Number of support vectors for each class.</p>
            </dd>
            <dt><strong>dual_coef_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class-1, n_SV]</span></dt>
            <dd><p class="first last">Coefficients of the support vector in the decision function.
            For multiclass, coefficient for all 1-vs-1 classifiers.
            The layout of the coefficients in the multiclass case is somewhat
            non-trivial. See the section about multi-class classification in the
            SVM section of the User Guide for details.</p>
            </dd>
            <dt><strong>coef_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class * (n_class-1) / 2, n_features]</span></dt>
            <dd><p class="first">Weights assigned to the features (coefficients in the primal
            problem). This is only available in the case of a linear kernel.</p>
            <p class="last"><cite>coef_</cite> is a readonly property derived from <cite>dual_coef_</cite> and
            <cite>support_vectors_</cite>.</p>
            </dd>
            <dt><strong>intercept_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class * (n_class-1) / 2]</span></dt>
            <dd><p class="first last">Constants in decision function.</p>
            </dd>
            <dt><strong>fit_status_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
            <dd><p class="first last">0 if correctly fitted, 1 otherwise (will raise warning)</p>
            </dd>
            <dt><strong>probA_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class * (n_class-1) / 2]</span></dt>
            <dd></dd>
            <dt><strong>probB_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array, shape = [n_class * (n_class-1) / 2]</span></dt>
            <dd><p class="first last">If probability=True, the parameters learned in Platt scaling to
            produce probability estimates from decision values. If
            probability=False, an empty array. Platt scaling uses the logistic
            function
            <code class="docutils literal"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">(1</span> <span class="pre">+</span> <span class="pre">exp(decision_value</span> <span class="pre">*</span> <span class="pre">probA_</span> <span class="pre">+</span> <span class="pre">probB_))</span></code>
            where <code class="docutils literal"><span class="pre">probA_</span></code> and <code class="docutils literal"><span class="pre">probB_</span></code> are learned from the dataset <a class="reference internal" href="#r20c70293ef72-2" id="id1">[2]</a>. For
            more information on the multiclass case and training procedure see
            section 8 of <a class="reference internal" href="#r20c70293ef72-1" id="id2">[1]</a>.</p>
            </dd>
            </dl>
            </td>
            </tr>
            </tbody>

            </table>
        </div>
    {% endautoescape %}

{% endblock %}