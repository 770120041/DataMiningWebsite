{% extends 'polls/base.html' %}

{% block content %}
    {#Base EXCEL NAV HERE#}
    {% autoescape off %}
        <div class = "container">


        <h1 class="text-primary">Random Forest Classifier:</h1>
        <p class="">
A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default).  </p>
        <p>
This class implements regularized logistic regression using the ‘liblinear’ library, ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).
        </p>
        <p>The ‘newton-cg’, ‘sag’, and ‘lbfgs’ solvers support only L2 regularization with primal formulation.
        The ‘liblinear’ solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.</p>

        <hr>

        <table class="docutils field-list" frame="void" rules="none">
                <colgroup><col class="field-name">
                <col class="field-body">
                </colgroup><tbody valign="top">
                <tr class="field-odd field"><th class="field-name"><h2 class="text-light  font-weight-bold">Parameters:</h2></th><td class="field-body"><dl class="first docutils">
                 <dt><strong>n_estimators</strong> <span class="classifier-delimiter">:</span> <span class="classifier">integer, optional (default=10)</span></dt>
                <dd><p class="first">The number of trees in the forest.</p>
                <div class="last versionchanged">
                <p><span class="versionmodified">Changed in version 0.20: </span>The default value of <code class="docutils literal"><span class="pre">n_estimators</span></code> will change from 10 in
                version 0.20 to 100 in version 0.22.</p>
                </div>
                </dd>
                <dt><strong>criterion</strong> <span class="classifier-delimiter">:</span> <span class="classifier">string, optional (default=”gini”)</span></dt>
                <dd><p class="first last">The function to measure the quality of a split. Supported criteria are
                “gini” for the Gini impurity and “entropy” for the information gain.
                Note: this parameter is tree-specific.</p>
                </dd>
                <dt><strong>max_depth</strong> <span class="classifier-delimiter">:</span> <span class="classifier">integer or None, optional (default=None)</span></dt>
                <dd><p class="first last">The maximum depth of the tree. If None, then nodes are expanded until
                all leaves are pure or until all leaves contain less than
                min_samples_split samples.</p>
                </dd>
                <dt><strong>min_samples_split</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=2)</span></dt>
                <dd><p class="first">The minimum number of samples required to split an internal node:</p>
                <ul class="simple">
                <li>If int, then consider <cite>min_samples_split</cite> as the minimum number.</li>
                <li>If float, then <cite>min_samples_split</cite> is a fraction and
                <cite>ceil(min_samples_split * n_samples)</cite> are the minimum
                number of samples for each split.</li>
                </ul>
                <div class="last versionchanged">
                <p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
                </div>
                </dd>
                <dt><strong>min_samples_leaf</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, float, optional (default=1)</span></dt>
                <dd><p class="first">The minimum number of samples required to be at a leaf node.
                A split point at any depth will only be considered if it leaves at
                least <code class="docutils literal"><span class="pre">min_samples_leaf</span></code> training samples in each of the left and
                right branches.  This may have the effect of smoothing the model,
                especially in regression.</p>
                <ul class="simple">
                <li>If int, then consider <cite>min_samples_leaf</cite> as the minimum number.</li>
                <li>If float, then <cite>min_samples_leaf</cite> is a fraction and
                <cite>ceil(min_samples_leaf * n_samples)</cite> are the minimum
                number of samples for each node.</li>
                </ul>
                <div class="last versionchanged">
                <p><span class="versionmodified">Changed in version 0.18: </span>Added float values for fractions.</p>
                </div>
                </dd>
                <dt><strong>min_weight_fraction_leaf</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
                <dd><p class="first last">The minimum weighted fraction of the sum total of weights (of all
                the input samples) required to be at a leaf node. Samples have
                equal weight when sample_weight is not provided.</p>
                </dd>
                <dt><strong>max_features</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, float, string or None, optional (default=”auto”)</span></dt>
                <dd><p class="first">The number of features to consider when looking for the best split:</p>
                <ul class="simple">
                <li>If int, then consider <cite>max_features</cite> features at each split.</li>
                <li>If float, then <cite>max_features</cite> is a fraction and
                <cite>int(max_features * n_features)</cite> features are considered at each
                split.</li>
                <li>If “auto”, then <cite>max_features=sqrt(n_features)</cite>.</li>
                <li>If “sqrt”, then <cite>max_features=sqrt(n_features)</cite> (same as “auto”).</li>
                <li>If “log2”, then <cite>max_features=log2(n_features)</cite>.</li>
                <li>If None, then <cite>max_features=n_features</cite>.</li>
                </ul>
                <p class="last">Note: the search for a split does not stop until at least one
                valid partition of the node samples is found, even if it requires to
                effectively inspect more than <code class="docutils literal"><span class="pre">max_features</span></code> features.</p>
                </dd>
                <dt><strong>max_leaf_nodes</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
                <dd><p class="first last">Grow trees with <code class="docutils literal"><span class="pre">max_leaf_nodes</span></code> in best-first fashion.
                Best nodes are defined as relative reduction in impurity.
                If None then unlimited number of leaf nodes.</p>
                </dd>
                <dt><strong>min_impurity_decrease</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float, optional (default=0.)</span></dt>
                <dd><p class="first">A node will be split if this split induces a decrease of the impurity
                greater than or equal to this value.</p>
                <p>The weighted impurity decrease equation is the following:</p>
                <div class="highlight-default" style="position: relative;"><div class="highlight"><pre><span></span><span class="n">N_t</span> <span class="o">/</span> <span class="n">N</span> <span class="o">*</span> <span class="p">(</span><span class="n">impurity</span> <span class="o">-</span> <span class="n">N_t_R</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">right_impurity</span>
                                    <span class="o">-</span> <span class="n">N_t_L</span> <span class="o">/</span> <span class="n">N_t</span> <span class="o">*</span> <span class="n">left_impurity</span><span class="p">)</span>
                </pre></div>
                </div>
                <p>where <code class="docutils literal"><span class="pre">N</span></code> is the total number of samples, <code class="docutils literal"><span class="pre">N_t</span></code> is the number of
                samples at the current node, <code class="docutils literal"><span class="pre">N_t_L</span></code> is the number of samples in the
                left child, and <code class="docutils literal"><span class="pre">N_t_R</span></code> is the number of samples in the right child.</p>
                <p><code class="docutils literal"><span class="pre">N</span></code>, <code class="docutils literal"><span class="pre">N_t</span></code>, <code class="docutils literal"><span class="pre">N_t_R</span></code> and <code class="docutils literal"><span class="pre">N_t_L</span></code> all refer to the weighted sum,
                if <code class="docutils literal"><span class="pre">sample_weight</span></code> is passed.</p>
                <div class="last versionadded">
                <p><span class="versionmodified">New in version 0.19.</span></p>
                </div>
                </dd>
                <dt><strong>min_impurity_split</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float, (default=1e-7)</span></dt>
                <dd><p class="first">Threshold for early stopping in tree growth. A node will split
                if its impurity is above the threshold, otherwise it is a leaf.</p>
                <div class="last deprecated">
                <p><span class="versionmodified">Deprecated since version 0.19: </span><code class="docutils literal"><span class="pre">min_impurity_split</span></code> has been deprecated in favor of
                <code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> in 0.19. The default value of
                <code class="docutils literal"><span class="pre">min_impurity_split</span></code> will change from 1e-7 to 0 in 0.23 and it
                will be removed in 0.25. Use <code class="docutils literal"><span class="pre">min_impurity_decrease</span></code> instead.</p>
                </div>
                </dd>
                <dt><strong>bootstrap</strong> <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional (default=True)</span></dt>
                <dd><p class="first last">Whether bootstrap samples are used when building trees. If False, the
                whole datset is used to build each tree.</p>
                </dd>
                <dt><strong>oob_score</strong> <span class="classifier-delimiter">:</span> <span class="classifier">bool (default=False)</span></dt>
                <dd><p class="first last">Whether to use out-of-bag samples to estimate
                the generalization accuracy.</p>
                </dd>
                <dt><strong>n_jobs</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
                <dd><p class="first last">The number of jobs to run in parallel for both <cite>fit</cite> and <cite>predict</cite>.
                <code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <a class="reference external" href="https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend" title="(in joblib v0.13.2)"><code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code></a> context.
                <code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <a class="reference internal" href="../../glossary.html#term-n-jobs"><span class="xref std std-term">Glossary</span></a>
                for more details.</p>
                </dd>
                <dt><strong>random_state</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional (default=None)</span></dt>
                <dd><p class="first last">If int, random_state is the seed used by the random number generator;
                If RandomState instance, random_state is the random number generator;
                If None, the random number generator is the RandomState instance used
                by <cite>np.random</cite>.</p>
                </dd>
                <dt><strong>verbose</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, optional (default=0)</span></dt>
                <dd><p class="first last">Controls the verbosity when fitting and predicting.</p>
                </dd>
                <dt><strong>warm_start</strong> <span class="classifier-delimiter">:</span> <span class="classifier">bool, optional (default=False)</span></dt>
                <dd><p class="first last">When set to <code class="docutils literal"><span class="pre">True</span></code>, reuse the solution of the previous call to fit
                and add more estimators to the ensemble, otherwise, just fit a whole
                new forest. See <a class="reference internal" href="../../glossary.html#term-warm-start"><span class="xref std std-term">the Glossary</span></a>.</p>
                </dd>
                <dt><strong>class_weight</strong> <span class="classifier-delimiter">:</span> <span class="classifier">dict, list of dicts, “balanced”, “balanced_subsample” or     None, optional (default=None)</span></dt>
                <dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
                If not given, all classes are supposed to have weight one. For
                multi-output problems, a list of dicts can be provided in the same
                order as the columns of y.</p>
                <p>Note that for multioutput (including multilabel) weights should be
                defined for each class of every column in its own dict. For example,
                for four-class multilabel classification weights should be
                [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of
                [{1:1}, {2:5}, {3:1}, {4:1}].</p>
                <p>The “balanced” mode uses the values of y to automatically adjust
                weights inversely proportional to class frequencies in the input data
                as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code></p>
                <p>The “balanced_subsample” mode is the same as “balanced” except that
                weights are computed based on the bootstrap sample for every tree
                grown.</p>
                <p>For multi-output, the weights of each column of y will be multiplied.</p>
                <p class="last">Note that these weights will be multiplied with sample_weight (passed
                through the fit method) if sample_weight is specified.</p>
                </dd>
                </dl>
                </td>
                </tr>
                <tr class="field-even field"><th class="field-name">Attributes:</th><td class="field-body"><dl class="first last docutils">
                <dt><strong>estimators_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">list of DecisionTreeClassifier</span></dt>
                <dd><p class="first last">The collection of fitted sub-estimators.</p>
                </dd>
                <dt><strong>classes_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_classes] or a list of such arrays</span></dt>
                <dd><p class="first last">The classes labels (single output problem), or a list of arrays of
                class labels (multi-output problem).</p>
                </dd>
                <dt><strong>n_classes_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int or list</span></dt>
                <dd><p class="first last">The number of classes (single output problem), or a list containing the
                number of classes for each output (multi-output problem).</p>
                </dd>
                <dt><strong>n_features_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
                <dd><p class="first last">The number of features when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</p>
                </dd>
                <dt><strong>n_outputs_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
                <dd><p class="first last">The number of outputs when <code class="docutils literal"><span class="pre">fit</span></code> is performed.</p>
                </dd>
                <dt><a class="reference internal" href="#sklearn.ensemble.RandomForestClassifier.feature_importances_" title="sklearn.ensemble.RandomForestClassifier.feature_importances_"><code class="xref py py-obj docutils literal"><span class="pre">feature_importances_</span></code></a> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_features]</span></dt>
                <dd><p class="first last">Return the feature importances (the higher, the more important the feature).</p>
                </dd>
                <dt><strong>oob_score_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
                <dd><p class="first last">Score of the training dataset obtained using an out-of-bag estimate.</p>
                </dd>
                <dt><strong>oob_decision_function_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array of shape = [n_samples, n_classes]</span></dt>
                <dd><p class="first last">Decision function computed with out-of-bag estimate on the training
                set. If n_estimators is small it might be possible that a data point
                was never left out during the bootstrap. In this case,
                <cite>oob_decision_function_</cite> might contain NaN.</p>
                </dd>
                </dl>
                </td>
                </tr>
                </tbody>
            </table>

        </div>
    {% endautoescape %}

{% endblock %}