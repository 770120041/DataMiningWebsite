{% extends 'polls/base.html' %}

{% block content %}
    {#Base EXCEL NAV HERE#}
    {% autoescape off %}
        <div class = "container">
        <h1 class="text-primary">KMeans:</h1>
            <p class="">K-Means clustering.
            </p>

        <hr>



       <table class="docutils field-list" frame="void" rules="none">
        <colgroup><col class="field-name">
        <col class="field-body">
        </colgroup><tbody valign="top">
        <tr class="field-odd field"><th class="field-name"><h2 class="text-light  font-weight-bold">Parameters:</h2></th><td class="field-body"><dl class="first docutils">
        <dt><strong>n_clusters</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, optional, default: 8</span></dt>
        <dd><p class="first last">The number of clusters to form as well as the number of
        centroids to generate.</p>
        </dd>
        <dt><strong>init</strong> <span class="classifier-delimiter">:</span> <span class="classifier">{‘k-means++’, ‘random’ or an ndarray}</span></dt>
        <dd><p class="first">Method for initialization, defaults to ‘k-means++’:</p>
        <p>‘k-means++’ : selects initial cluster centers for k-mean
        clustering in a smart way to speed up convergence. See section
        Notes in k_init for more details.</p>
        <p>‘random’: choose k observations (rows) at random from data for
        the initial centroids.</p>
        <p class="last">If an ndarray is passed, it should be of shape (n_clusters, n_features)
        and gives the initial centers.</p>
        </dd>
        <dt><strong>n_init</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 10</span></dt>
        <dd><p class="first last">Number of time the k-means algorithm will be run with different
        centroid seeds. The final results will be the best output of
        n_init consecutive runs in terms of inertia.</p>
        </dd>
        <dt><strong>max_iter</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 300</span></dt>
        <dd><p class="first last">Maximum number of iterations of the k-means algorithm for a
        single run.</p>
        </dd>
        <dt><strong>tol</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float, default: 1e-4</span></dt>
        <dd><p class="first last">Relative tolerance with regards to inertia to declare convergence</p>
        </dd>
        <dt><strong>precompute_distances</strong> <span class="classifier-delimiter">:</span> <span class="classifier">{‘auto’, True, False}</span></dt>
        <dd><p class="first">Precompute distances (faster but takes more memory).</p>
        <p>‘auto’ : do not precompute distances if n_samples * n_clusters &gt; 12
        million. This corresponds to about 100MB overhead per job using
        double precision.</p>
        <p>True : always precompute distances</p>
        <p class="last">False : never precompute distances</p>
        </dd>
        <dt><strong>verbose</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, default 0</span></dt>
        <dd><p class="first last">Verbosity mode.</p>
        </dd>
        <dt><strong>random_state</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None (default)</span></dt>
        <dd><p class="first last">Determines random number generation for centroid initialization. Use
        an int to make the randomness deterministic.
        See <a class="reference internal" href="../../glossary.html#term-random-state"><span class="xref std std-term">Glossary</span></a>.</p>
        </dd>
        <dt><strong>copy_x</strong> <span class="classifier-delimiter">:</span> <span class="classifier">boolean, optional</span></dt>
        <dd><p class="first last">When pre-computing distances it is more numerically accurate to center
        the data first.  If copy_x is True (default), then the original data is
        not modified, ensuring X is C-contiguous.  If False, the original data
        is modified, and put back before the function returns, but small
        numerical differences may be introduced by subtracting and then adding
        the data mean, in this case it will also not ensure that data is
        C-contiguous which may cause a significant slowdown.</p>
        </dd>
        <dt><strong>n_jobs</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
        <dd><p class="first">The number of jobs to use for the computation. This works by computing
        each of the n_init runs in parallel.</p>
        <p class="last"><code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <a class="reference external" href="https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend" title="(in joblib v0.13.2)"><code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code></a> context.
        <code class="docutils literal"><span class="pre">-1</span></code> means using all processors. See <a class="reference internal" href="../../glossary.html#term-n-jobs"><span class="xref std std-term">Glossary</span></a>
        for more details.</p>
        </dd>
        <dt><strong>algorithm</strong> <span class="classifier-delimiter">:</span> <span class="classifier">“auto”, “full” or “elkan”, default=”auto”</span></dt>
        <dd><p class="first last">K-means algorithm to use. The classical EM-style algorithm is “full”.
        The “elkan” variation is more efficient by using the triangle
        inequality, but currently doesn’t support sparse data. “auto” chooses
        “elkan” for dense data and “full” for sparse data.</p>
        </dd>
        </dl>
        </td>
        </tr>
        <tr class="field-even field"><th class="field-name">Attributes:</th><td class="field-body"><dl class="first last docutils">
        <dt><strong>cluster_centers_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">array, [n_clusters, n_features]</span></dt>
        <dd><p class="first last">Coordinates of cluster centers. If the algorithm stops before fully
        converging (see <code class="docutils literal"><span class="pre">tol</span></code> and <code class="docutils literal"><span class="pre">max_iter</span></code>), these will not be
        consistent with <code class="docutils literal"><span class="pre">labels_</span></code>.</p>
        </dd>
        <dt><strong>labels_ :</strong></dt>
        <dd><p class="first last">Labels of each point</p>
        </dd>
        <dt><strong>inertia_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float</span></dt>
        <dd><p class="first last">Sum of squared distances of samples to their closest cluster center.</p>
        </dd>
        <dt><strong>n_iter_</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int</span></dt>
        <dd><p class="first last">Number of iterations run.</p>
        </dd>
        </dl>
        </td>
        </tr>
        </tbody>
        </table>

        </div>
    {% endautoescape %}

{% endblock %}