{% extends 'polls/base.html' %}

{% block content %}
    {#Base EXCEL NAV HERE#}
    {% autoescape off %}
        <div class = "container">
        <h1 class="text-primary">Logistic Regression:</h1>
        <p class="">Logistic Regression (aka logit, MaxEnt) classifier.

In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the ‘multi_class’ option is set to ‘ovr’, and uses the cross- entropy loss if the ‘multi_class’ option is set to ‘multinomial’. (Currently the ‘multinomial’ option is supported only by the ‘lbfgs’, ‘sag’ and ‘newton-cg’ solvers.)
        </p>
        <p>
This class implements regularized logistic regression using the ‘liblinear’ library, ‘newton-cg’, ‘sag’ and ‘lbfgs’ solvers. It can handle both dense and sparse input. Use C-ordered arrays or CSR matrices containing 64-bit floats for optimal performance; any other input format will be converted (and copied).
        </p>
        <p>The ‘newton-cg’, ‘sag’, and ‘lbfgs’ solvers support only L2 regularization with primal formulation.
        The ‘liblinear’ solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty.</p>

        <hr>

        <table class="docutils field-list" frame="void" rules="none">
            <colgroup><col class="field-name">
            <col class="field-body">
            </colgroup><tbody valign="top">
            <tr class="field-odd field"><th class="field-name"><h2 class="text-light  font-weight-bold">Parameters:</h2></th><td class="field-body"><dl class="first docutils">
          <dt><strong>penalty</strong> <span class="classifier-delimiter">:</span> <span class="classifier">str, ‘l1’ or ‘l2’, default: ‘l2’</span></dt>
        <dd><p class="first">Used to specify the norm used in the penalization. The ‘newton-cg’,
        ‘sag’ and ‘lbfgs’ solvers support only l2 penalties.</p>
        <div class="last versionadded">
        <p><span class="versionmodified">New in version 0.19: </span>l1 penalty with SAGA solver (allowing ‘multinomial’ + L1)</p>
        </div>
        </dd>
        <dt><strong>dual</strong> <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span></dt>
        <dd><p class="first last">Dual or primal formulation. Dual formulation is only implemented for
        l2 penalty with liblinear solver. Prefer dual=False when
        n_samples &gt; n_features.</p>
        </dd>
        <dt><strong>tol</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float, default: 1e-4</span></dt>
        <dd><p class="first last">Tolerance for stopping criteria.</p>
        </dd>
        <dt><strong>C</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float, default: 1.0</span></dt>
        <dd><p class="first last">Inverse of regularization strength; must be a positive float.
        Like in support vector machines, smaller values specify stronger
        regularization.</p>
        </dd>
        <dt><strong>fit_intercept</strong> <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: True</span></dt>
        <dd><p class="first last">Specifies if a constant (a.k.a. bias or intercept) should be
        added to the decision function.</p>
        </dd>
        <dt><strong>intercept_scaling</strong> <span class="classifier-delimiter">:</span> <span class="classifier">float, default 1.</span></dt>
        <dd><p class="first">Useful only when the solver ‘liblinear’ is used
        and self.fit_intercept is set to True. In this case, x becomes
        [x, self.intercept_scaling],
        i.e. a “synthetic” feature with constant value equal to
        intercept_scaling is appended to the instance vector.
        The intercept becomes <code class="docutils literal"><span class="pre">intercept_scaling</span> <span class="pre">*</span> <span class="pre">synthetic_feature_weight</span></code>.</p>
        <p class="last">Note! the synthetic feature weight is subject to l1/l2 regularization
        as all other features.
        To lessen the effect of regularization on synthetic feature weight
        (and therefore on the intercept) intercept_scaling has to be increased.</p>
        </dd>
        <dt><strong>class_weight</strong> <span class="classifier-delimiter">:</span> <span class="classifier">dict or ‘balanced’, default: None</span></dt>
        <dd><p class="first">Weights associated with classes in the form <code class="docutils literal"><span class="pre">{class_label:</span> <span class="pre">weight}</span></code>.
        If not given, all classes are supposed to have weight one.</p>
        <p>The “balanced” mode uses the values of y to automatically adjust
        weights inversely proportional to class frequencies in the input data
        as <code class="docutils literal"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code>.</p>
        <p>Note that these weights will be multiplied with sample_weight (passed
        through the fit method) if sample_weight is specified.</p>
        <div class="last versionadded">
        <p><span class="versionmodified">New in version 0.17: </span><em>class_weight=’balanced’</em></p>
        </div>
        </dd>
        <dt><strong>random_state</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, RandomState instance or None, optional, default: None</span></dt>
        <dd><p class="first last">The seed of the pseudo random number generator to use when shuffling
        the data.  If int, random_state is the seed used by the random number
        generator; If RandomState instance, random_state is the random number
        generator; If None, the random number generator is the RandomState
        instance used by <cite>np.random</cite>. Used when <code class="docutils literal"><span class="pre">solver</span></code> == ‘sag’ or
        ‘liblinear’.</p>
        </dd>
        <dt><strong>solver</strong> <span class="classifier-delimiter">:</span> <span class="classifier">str, {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’},              default: ‘liblinear’.</span></dt>
        <dd><p class="first">Algorithm to use in the optimization problem.</p>
        <ul class="simple">
        <li>For small datasets, ‘liblinear’ is a good choice, whereas ‘sag’ and
        ‘saga’ are faster for large ones.</li>
        <li>For multiclass problems, only ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’
        handle multinomial loss; ‘liblinear’ is limited to one-versus-rest
        schemes.</li>
        <li>‘newton-cg’, ‘lbfgs’ and ‘sag’ only handle L2 penalty, whereas
        ‘liblinear’ and ‘saga’ handle L1 penalty.</li>
        </ul>
        <p>Note that ‘sag’ and ‘saga’ fast convergence is only guaranteed on
        features with approximately the same scale. You can
        preprocess the data with a scaler from sklearn.preprocessing.</p>
        <div class="versionadded">
        <p><span class="versionmodified">New in version 0.17: </span>Stochastic Average Gradient descent solver.</p>
        </div>
        <div class="versionadded">
        <p><span class="versionmodified">New in version 0.19: </span>SAGA solver.</p>
        </div>
        <div class="last versionchanged">
        <p><span class="versionmodified">Changed in version 0.20: </span>Default will change from ‘liblinear’ to ‘lbfgs’ in 0.22.</p>
        </div>
        </dd>
        <dt><strong>max_iter</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 100</span></dt>
        <dd><p class="first last">Useful only for the newton-cg, sag and lbfgs solvers.
        Maximum number of iterations taken for the solvers to converge.</p>
        </dd>
        <dt><strong>multi_class</strong> <span class="classifier-delimiter">:</span> <span class="classifier">str, {‘ovr’, ‘multinomial’, ‘auto’}, default: ‘ovr’</span></dt>
        <dd><p class="first">If the option chosen is ‘ovr’, then a binary problem is fit for each
        label. For ‘multinomial’ the loss minimised is the multinomial loss fit
        across the entire probability distribution, <em>even when the data is
        binary</em>. ‘multinomial’ is unavailable when solver=’liblinear’.
        ‘auto’ selects ‘ovr’ if the data is binary, or if solver=’liblinear’,
        and otherwise selects ‘multinomial’.</p>
        <div class="versionadded">
        <p><span class="versionmodified">New in version 0.18: </span>Stochastic Average Gradient descent solver for ‘multinomial’ case.</p>
        </div>
        <div class="last versionchanged">
        <p><span class="versionmodified">Changed in version 0.20: </span>Default will change from ‘ovr’ to ‘auto’ in 0.22.</p>
        </div>
        </dd>
        <dt><strong>verbose</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int, default: 0</span></dt>
        <dd><p class="first last">For the liblinear and lbfgs solvers set verbose to any positive
        number for verbosity.</p>
        </dd>
        <dt><strong>warm_start</strong> <span class="classifier-delimiter">:</span> <span class="classifier">bool, default: False</span></dt>
        <dd><p class="first">When set to True, reuse the solution of the previous call to fit as
        initialization, otherwise, just erase the previous solution.
        Useless for liblinear solver. See <a class="reference internal" href="../../glossary.html#term-warm-start"><span class="xref std std-term">the Glossary</span></a>.</p>
        <div class="last versionadded">
        <p><span class="versionmodified">New in version 0.17: </span><em>warm_start</em> to support <em>lbfgs</em>, <em>newton-cg</em>, <em>sag</em>, <em>saga</em> solvers.</p>
        </div>
        </dd>
        <dt><strong>n_jobs</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int or None, optional (default=None)</span></dt>
        <dd><p class="first last">Number of CPU cores used when parallelizing over classes if
        multi_class=’ovr’”. This parameter is ignored when the <code class="docutils literal"><span class="pre">solver</span></code> is
        set to ‘liblinear’ regardless of whether ‘multi_class’ is specified or
        not. <code class="docutils literal"><span class="pre">None</span></code> means 1 unless in a <a class="reference external" href="https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend" title="(in joblib v0.13.2)"><code class="xref py py-obj docutils literal"><span class="pre">joblib.parallel_backend</span></code></a>
        context. <code class="docutils literal"><span class="pre">-1</span></code> means using all processors.
        See <a class="reference internal" href="../../glossary.html#term-n-jobs"><span class="xref std std-term">Glossary</span></a> for more details.</p>
        </dd>
        </dl>
        </td>
         </tr>
            </tbody>

            </table>
        </div>

    {% endautoescape %}

{% endblock %}